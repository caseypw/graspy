{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Gaussian Mixture Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is a foundational data analysis task, where members of the data set are sorted into groups or \"clusters\" according to measured similarities between the objects. According to some quantitative criteria, members of the same cluster are similar and members of distinct clusters are different.\n",
    "\n",
    "The Automatic Gaussian Mixture Model (AutoGMM) is a clustering algorithm that uses Sklearn's hierarchical agglomerative clustering and then Gaussian mixtured model (GMM) fitting. Different combinations of agglomeration, GMM, and cluster numbers are used in the algorithm, and the clustering with the best selection criterion (bic/aic) is chosen.\n",
    "\n",
    "The Gaussian mixture model (GMM) is a statistical model of clustered data that, simply put, is a composition of multiple normal distributions. Each cluster has a weight $w_k$ assigned to it, and the combined probability distribution, $f(x)$, is of the form:\n",
    "\n",
    "$f(x) = \\sum\\limits_{k = 1}^K {w_{k}f_{k}(x)} = \\sum\\limits_{k = 1}^K {\\frac{w_{k}}{(2\\pi)^{\\frac{d}{2}}|\\sum_{k}|^{-\\frac{1}{2}}}e^{[\\frac{1}{2}(x - \\mu_{k})^{T}\\sum_{k}^{-1}(x - \\mu_{k})]}}$\n",
    "\n",
    "where $k$ is the total number of clusters and $d$ is the dimensionality of the data.\n",
    "\n",
    "Expectation Maximization (EM) algorithms are then run to estimate model parameters and the fitted GMM is used to cluster the data.  \n",
    "$$   \n",
    "$$  \n",
    "Let's look at an example using synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from graspologic.cluster.autogmm import AutoGMMCluster\n",
    "\n",
    "# Synthetic data\n",
    "\n",
    "# Dim 1\n",
    "class_1 = np.random.randn(150, 1)\n",
    "class_2 = 2 + np.random.randn(150, 1)\n",
    "dim_1 = np.vstack((class_1, class_2))\n",
    "\n",
    "# Dim 2\n",
    "class_3 = np.random.randn(150, 1)\n",
    "class_4 = 2 + np.random.randn(150, 1)\n",
    "dim_2 = np.vstack((class_3, class_4))\n",
    "\n",
    "X = np.hstack((dim_1, dim_2))\n",
    "\n",
    "# Labels\n",
    "label_1 = np.zeros((150, 1))\n",
    "label_2 = 1 + label_1\n",
    "\n",
    "c = np.vstack((label_1, label_2)).reshape(300,)\n",
    "\n",
    "# Fit model\n",
    "autogmm_ = AutoGMMCluster(affinity='all',linkage='all',covariance_type='all')\n",
    "\n",
    "# Estimated Labels\n",
    "c_hat_autogmm = autogmm_.fit_predict(X,c)"
   ]
  },
  {
   "source": [
    "We can compare our method to the existing implementation of GMM in Sklearn. Our method expands upon the existing Sklearn framework by allowing the user to automatically find the optimal parameters for a Gaussian mixture model and achieve the best clustering possible. In particular, the ideal number of components `n_components` is output by AutoGMM. If we create a GMM model with our parameters using Sklearn, we will see an optimal fit."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Ideal parameters from AutoGMM\n",
    "n_comp = autogmm_.n_components_\n",
    "\n",
    "cov = autogmm_.covariance_type_\n",
    "\n",
    "# Have to provide exact number of optimum components apriori\n",
    "gmm_ = mixture.GaussianMixture(n_components=n_comp, covariance_type=cov)\n",
    "gmm_default_ = mixture.GaussianMixture(2)\n",
    "\n",
    "# Predicted Labels\n",
    "c_hat_gmm = gmm_.fit_predict(X)\n",
    "c_hat_gmm_default = gmm_default_.fit_predict(X)\n",
    "\n",
    "# Function to remap labels for direct comparison\n",
    "def remap_labels(y_true, y_pred):\n",
    "\n",
    "    confusion_mat = confusion_matrix(y_true, y_pred)\n",
    "    row_inds, col_inds = linear_sum_assignment(confusion_mat, maximize=True)\n",
    "    label_map = dict(zip(col_inds, row_inds))\n",
    "    remapped_y_pred = np.vectorize(label_map.get)(y_pred)\n",
    "    \n",
    "    return remapped_y_pred\n",
    "\n",
    "# Remap Predicted labels\n",
    "c_hat_autogmm = remap_labels(c, c_hat_autogmm)\n",
    "c_hat_gmm = remap_labels(c, c_hat_gmm)\n",
    "c_hat_gmm_default = remap_labels(c, c_hat_gmm_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting Synthetic clusters\n",
    "import matplotlib as mp\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_context(\"talk\", font_scale=1.10)\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "# Labelings\n",
    "c_list = ['red', 'green', 'blue','orange','purple','yellow','gray']\n",
    "\n",
    "# Figure\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "# Synthetic True Clustering\n",
    "plt.subplot(2, 2, (1,1))\n",
    "plt.title('True Clustering',fontsize=24,fontweight='bold')\n",
    "max_c = int(np.max(c))\n",
    "plt.scatter(X[:,0],X[:,1],c=c,cmap=mp.colors.ListedColormap(c_list[0:max_c+1]))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "# Synthetic AutoGMM Model\n",
    "plt.subplot(2, 2, (2,2))\n",
    "plt.title('AutoGMM Clustering',fontsize=24,fontweight='bold')\n",
    "max_c = int(np.max(c_hat_autogmm))\n",
    "plt.scatter(X[:,0],X[:,1],c=c_hat_autogmm,cmap=mp.colors.ListedColormap(c_list[0:max_c+1]))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "# Synthetic Optimal SKlearn Model\n",
    "plt.subplot(2, 2, (3,3))\n",
    "plt.title('Optimal SKlearn Clustering',fontsize=24,fontweight='bold')\n",
    "max_c = int(np.max(c_hat_gmm))\n",
    "plt.scatter(X[:,0],X[:,1],c=c_hat_gmm,cmap=mp.colors.ListedColormap(c_list[0:max_c+1]))\n",
    "plt.xlabel('First Dimension',fontsize=24)\n",
    "plt.ylabel('Second Dimension',fontsize=24)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "# Synthetic Default SKlearn Model\n",
    "plt.subplot(2, 2, (4,4))\n",
    "plt.title('Default SKlearn Clustering',fontsize=24,fontweight='bold')\n",
    "max_c = int(np.max(c_hat_gmm_default))\n",
    "plt.scatter(X[:,0],X[:,1],c=c_hat_gmm_default,cmap=mp.colors.ListedColormap(c_list[0:max_c+1]))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}